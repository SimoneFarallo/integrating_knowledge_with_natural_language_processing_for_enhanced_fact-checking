{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import re \n",
    "import pickle\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel , AutoTokenizer, ErnieModel, AutoModel\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1000) \n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kb = pd.read_csv(r'df_kb_generation.csv')\n",
    "df_kb[['new_claim','new_expl']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean text\n",
    "# Rimuovi le parti precedenti a :\\r\\n\n",
    "df_kb['new_claim'] = df_kb['new_claim'].apply(lambda x: re.sub(r'.*:\\r\\n', '', x)) \n",
    "df_kb['new_expl'] = df_kb['new_expl'].apply(lambda x: re.sub(r'.*:\\r\\n', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean text\n",
    "# Rimuovi le stringhe esattamente uguali a \\r\\n\n",
    "df_kb['new_claim'] = df_kb['new_claim'].replace('\"\"\"\\r\\n', '', regex=True) \n",
    "df_kb['new_claim'] = df_kb['new_claim'].replace('\\r\\n\"\"\"', '', regex=True)\n",
    "df_kb['new_expl'] = df_kb['new_expl'].replace('\"\"\"\\r\\n', '', regex=True) \n",
    "df_kb['new_expl'] = df_kb['new_expl'].replace('\\r\\n\"\"\"', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kb[['new_claim','new_expl']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claim = df_kb[[\"new_claim\",\"label\"]]\n",
    "df_claim.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(input, model=\"text-embedding-3-small\", encoding_format = \"float\"):\n",
    "  obj = client.embeddings.create(\n",
    "    model=model,\n",
    "    input=input,\n",
    "    encoding_format=encoding_format\n",
    "  )\n",
    "  return obj.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = df_claim[\"new_claim\"].tolist()\n",
    "vectors_list = [get_embedding(claim) for claim in claims]\n",
    "vectors = np.array(vectors_list, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings_claims_ADA.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explaination = df_kb[[\"new_expl\",\"label\"]]\n",
    "df_explaination.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainations = df_explaination[\"new_expl\"].tolist()\n",
    "vectors_list_expl = [get_embedding(explaination) for explaination in explainations]\n",
    "vectors_expl = np.array(vectors_list_expl, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings_explaination_ADA.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectors_expl,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dimension = vectors.shape[1]\n",
    "index = faiss.IndexFlatL2(vector_dimension)\n",
    "faiss.normalize_L2(vectors)\n",
    "index.add(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializzazione del tokenizer e del modello\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(sentence):\n",
    "    # Tokenizzazione: conversione della frase in token e aggiunta dei token speciali\n",
    "    encoded_input = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    # Calcolo dell'embedding\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    # Estrazione dell'embedding del token CLS (puoi anche usare un approccio diverso, ad es. media degli embedding)\n",
    "    embedding = output.last_hidden_state[:,0,:].numpy()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = df_kb[\"new_claim\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo degli embedding per ogni frase nel dataset\n",
    "vectors_list = [get_bert_embedding(claim) for claim in claims]\n",
    "vectors = np.vstack(vectors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings_claims_RoBERT.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = df_kb[\"new_expl\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo degli embedding per ogni frase nel dataset\n",
    "vectors_list = [get_bert_embedding(explanation) for explanation in explanations]\n",
    "vectors = np.vstack(vectors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings_explanation_RoBERT.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## ERNIE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-base-en\")\n",
    "model = ErnieModel.from_pretrained(\"nghuyong/ernie-2.0-base-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(sentence):\n",
    "    # Tokenizzazione: conversione della frase in token e aggiunta dei token speciali\n",
    "    encoded_input = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    # Calcolo dell'embedding\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    # Estrazione dell'embedding del token CLS (puoi anche usare un approccio diverso, ad es. media degli embedding)\n",
    "    embedding = output.last_hidden_state[:,0,:].numpy()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = df_kb[\"new_claim\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo degli embedding per ogni frase nel dataset\n",
    "vectors_list = [get_bert_embedding(claim) for claim in claims]\n",
    "vectors = np.vstack(vectors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings_claims_ERNIE.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = df_kb[\"new_expl\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo degli embedding per ogni frase nel dataset\n",
    "vectors_list = [get_bert_embedding(explanation) for explanation in explanations]\n",
    "vectors = np.vstack(vectors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings_explanation_ERNIE.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distil-Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(sentence):\n",
    "    encoded_input = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    # Calcolo dell'embedding\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    # Estrazione dell'embedding del token CLS (puoi anche usare un approccio diverso, ad es. media degli embedding)\n",
    "    embedding = output.last_hidden_state[:,0,:].numpy()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = df_kb[\"new_claim\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo degli embedding per ogni frase nel dataset\n",
    "vectors_list = [get_bert_embedding(claim) for claim in claims]\n",
    "vectors = np.vstack(vectors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings_claims_distilBERT.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = df_kb[\"new_expl\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo degli embedding per ogni frase nel dataset\n",
    "vectors_list = [get_bert_embedding(explanation) for explanation in explanations]\n",
    "vectors = np.vstack(vectors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings_explanation_distilBERT.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
