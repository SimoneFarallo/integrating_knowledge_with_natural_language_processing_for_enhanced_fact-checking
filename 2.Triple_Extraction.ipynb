{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triple Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stanza spacy https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modello spacy\n",
    "#!python -m spacy download en_core_web_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import BertTokenizer, BertModel, AutoModelForSeq2SeqLM\n",
    "import stanza\n",
    "from stanza.server import CoreNLPClient\n",
    "import os\n",
    "import random\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.REBEL\n",
    "\n",
    "https://huggingface.co/Babelscape/rebel-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")\n",
    "gen_kwargs = {\n",
    "    \"max_length\": 256,\n",
    "    \"length_penalty\": 0,\n",
    "    \"num_beams\": 3,\n",
    "    \"num_return_sequences\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply\n",
    "def apply_extraction(row):\n",
    "    # Tokenize the text\n",
    "    model_inputs = tokenizer(row['claim'], max_length=256, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "\n",
    "\n",
    "    # Generate\n",
    "    generated_tokens = model.generate(\n",
    "        model_inputs[\"input_ids\"].to(model.device),\n",
    "        attention_mask=model_inputs[\"attention_mask\"].to(model.device),\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "    # Extract triplets\n",
    "    triplets = []\n",
    "    for idx, sentence in enumerate(decoded_preds):\n",
    "        triplets.extend(extract_triplets(sentence))\n",
    "\n",
    "    return triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row in the DataFrame\n",
    "df['triplets'] = df.apply(apply_extraction, axis=1)\n",
    "\n",
    "# Display the results\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Claim : {row['claim']}\")\n",
    "    print(f\"Triplets: {row['triplets']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.CORENLP\n",
    "\n",
    "https://stanfordnlp.github.io/CoreNLP/openie.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NO_PROXY='localhost'\n",
      "env: no_proxy='localhost'\n"
     ]
    }
   ],
   "source": [
    "%env NO_PROXY='localhost'\n",
    "%env no_proxy='localhost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"ls\" non � riconosciuto come comando interno o esterno,\n",
      " un programma eseguibile o un file batch.\n"
     ]
    }
   ],
   "source": [
    "# Examine the CoreNLP installation folder to make sure the installation is successful\n",
    "!ls $CORENLP_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f865dc033146a29da11e3cf1bee86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 12:51:43 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-02-11 12:51:44 INFO: File exists: C:\\Users\\c.farallo\\stanza_resources\\en\\default.zip\n",
      "2024-02-11 12:51:48 INFO: Finished downloading models and saved to C:\\Users\\c.farallo\\stanza_resources.\n",
      "2024-02-11 12:51:48 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5bba5511bc4f38935f394b478eb852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 12:51:49 INFO: Loading these models for language: en (English):\n",
      "==============================\n",
      "| Processor | Package        |\n",
      "------------------------------\n",
      "| tokenize  | genia          |\n",
      "| pos       | genia_nocharlm |\n",
      "| lemma     | genia_nocharlm |\n",
      "| depparse  | genia_nocharlm |\n",
      "==============================\n",
      "\n",
      "2024-02-11 12:51:49 INFO: Using device: cpu\n",
      "2024-02-11 12:51:49 INFO: Loading: tokenize\n",
      "2024-02-11 12:51:49 INFO: Loading: pos\n",
      "2024-02-11 12:51:49 INFO: Loading: lemma\n",
      "2024-02-11 12:51:49 INFO: Loading: depparse\n",
      "2024-02-11 12:51:49 INFO: Done loading processors!\n",
      "2024-02-11 12:51:49 WARNING: Directory ./corenlp already exists. Please install CoreNLP to a new directory.\n",
      "2024-02-11 12:51:50 INFO: Writing properties to tmp file: corenlp_server-8fd2bd047882487b.props\n"
     ]
    }
   ],
   "source": [
    "# Scarica e inizializza il modello e il server CoreNLP una sola volta\n",
    "stanza.download('en')\n",
    "nlp_model = stanza.Pipeline('en', package='genia')\n",
    "corenlp_dir = './corenlp'\n",
    "stanza.install_corenlp(dir=corenlp_dir)\n",
    "os.environ[\"CORENLP_HOME\"] = corenlp_dir\n",
    "\n",
    "# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\n",
    "client = CoreNLPClient(\n",
    "    annotators=['openie'],\n",
    "    memory='4G',\n",
    "    endpoint='http://localhost:9006',\n",
    "    be_quiet=True)\n",
    "\n",
    "# Definisci la funzione extract_openie_triplets utilizzando le variabili globali\n",
    "def extract_openie_triplets(text):\n",
    "    # Utilizza il modello e il server CoreNLP globali\n",
    "    document = client.annotate(text, output_format='json')\n",
    "\n",
    "    # Estrai le triplette OpenIE dal documento annotato\n",
    "    triples = []\n",
    "    for sentence in document['sentences']:\n",
    "      for triple in sentence['openie']:\n",
    "        triples.append({\n",
    "           'subject': triple['subject'],\n",
    "           'relation': triple['relation'],\n",
    "            'object': triple['object']\n",
    "        })\n",
    "    print(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Spacy\n",
    "\n",
    "https://spacy.io/usage/linguistic-features#pos-tagging\n",
    "\n",
    "https://spacy.io/api/token#attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP dobj X.X. False False\n",
      "startup startup NOUN NN dep xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prova del codice\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text = str(row['claim'])\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Estrai soggetto, predicato e oggetto\n",
    "    subjects = [token.text for token in doc if token.dep_ in [\"nsubj\", \"nsubjpass\"]]\n",
    "\n",
    "    # Estendi la condizione per l'estrazione dei predicati\n",
    "    predicates = [token.text for token in doc if token.dep_ in [\"ROOT\", \"aux\"]]\n",
    "\n",
    "    objects = [token.text for token in doc if token.dep_ in [\"dobj\", \"attr\", \"prep\"]]\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(f\"Claim: {text} - Subjects: {subjects}, Predicates: {predicates}, Objects: {objects}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
